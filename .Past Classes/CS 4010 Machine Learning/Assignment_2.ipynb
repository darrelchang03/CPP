{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Assignment 2 - Using a Neural Network to fit the California Housing data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1600899846874,
     "user": {
      "displayName": "Hao Ji",
      "photoUrl": "",
      "userId": "12290693972539811867"
     },
     "user_tz": 420
    },
    "id": "Q71FjRQDhMdI",
    "outputId": "ceae955b-2e3c-44ca-f5a2-967b026f0d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 13)\n"
     ]
    }
   ],
   "source": [
    "# California Housing dataset\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import torch\n",
    "from torch import save, load\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# load data from csv file\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\", \"housing.csv\")\n",
    "housing = pd.read_csv('housing.csv')\n",
    "\n",
    "# Using the setting inplace=False, drop() creates a copy of the data and does not affect housing dataset\n",
    "housing_data = housing.drop(\"median_house_value\", axis=1, inplace=False)\n",
    "housing_target = housing[\"median_house_value\"].copy()\n",
    "feature_names = list(housing_data.columns)\n",
    "\n",
    "#  Transformation pipeline at https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, feature_names[:-1]),\n",
    "    ('cat', OneHotEncoder(), [feature_names[-1]]),\n",
    "])\n",
    "\n",
    "housing_preprocessed = full_pipeline.fit_transform(housing_data)\n",
    "\n",
    "print(housing_preprocessed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_preprocessed\n",
    "y = housing_target.to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (for comparsion) Using scikit-learn's Linear Regression model to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1600899872762,
     "user": {
      "displayName": "Hao Ji",
      "photoUrl": "",
      "userId": "12290693972539811867"
     },
     "user_tz": 420
    },
    "id": "kpzf_PWRtXz3",
    "outputId": "c5871fea-80b2-444d-cf85-bf91a87b072b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training error : 0.646\n",
      "model testing error: 0.642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# documentation at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"model training error : %.3f\" % lr.score(X_train, y_train))\n",
    "print(\"model testing error: %.3f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\AppData\\Local\\Temp\\ipykernel_20896\\2098408231.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train)\n",
      "C:\\Users\\darre\\AppData\\Local\\Temp\\ipykernel_20896\\2098408231.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test)\n",
      "C:\\Users\\darre\\AppData\\Local\\Temp\\ipykernel_20896\\2098408231.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train)\n",
      "C:\\Users\\darre\\AppData\\Local\\Temp\\ipykernel_20896\\2098408231.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test)\n"
     ]
    }
   ],
   "source": [
    "# Converting datasets to tensors\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1867,
     "status": "ok",
     "timestamp": 1600899847862,
     "user": {
      "displayName": "Hao Ji",
      "photoUrl": "",
      "userId": "12290693972539811867"
     },
     "user_tz": 420
    },
    "id": "v5ScwbRUuT8V"
   },
   "source": [
    "# (Task 1 of Assignment 2): Using PyTorch nn.Sequential() to build a neural network to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     46\u001b[0m \u001b[39m# Calculate gradients (backward pass)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     49\u001b[0m \u001b[39m# Update weights\u001b[39;00m\n\u001b[0;32m     50\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Design model - input, output, forward pass with different layers\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size,input_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size //2 ,input_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size // 4 , 1),\n",
    "        )\n",
    "        self.double()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "model = NeuralNet(13)\n",
    "\n",
    "# 2) Create loss and optimizer\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 3) Training Loop\n",
    "#        Forward (compute predictions and loss)\n",
    "#        Backward (compute gradients)\n",
    "#        Update weights\n",
    "\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    # predict y (forward pass)\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_train, y_pred)\n",
    "\n",
    "    # Clear gradients for next loop\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate gradients (backward pass)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress every x number of epochs\n",
    "    if ((epoch+1) % 100 == 0):\n",
    "        print(f'Epoch: {epoch+1} loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "with open('model_state.pt', 'wb') as f:\n",
    "    save(model.state_dict(), f)\n",
    "\n",
    "with open('model_state.pt', 'rb') as f:\n",
    "    model.load_state_dict(load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Task 2 of Assignment 2): Subclassing nn.Module to build a neural network (the same network structure as Task 1) to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 loss: 43095924076.973465\n",
      "Epoch: 20 loss: 33209436758.258667\n",
      "Epoch: 30 loss: 26609139011.62903\n",
      "Epoch: 40 loss: 22202727620.021893\n",
      "Epoch: 50 loss: 19260972248.152504\n",
      "Epoch: 60 loss: 17297032910.93914\n",
      "Epoch: 70 loss: 15985891353.372066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     47\u001b[0m \u001b[39m# Calculate gradients (backward pass)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     50\u001b[0m \u001b[39m# Update weights\u001b[39;00m\n\u001b[0;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Design model - input, output, forward pass with different layers\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size // 2)  # First hidden layer\n",
    "        self.relu1 = nn.ReLU()  # ReLU activation function\n",
    "        self.fc2 = nn.Linear(input_size // 2, input_size // 4)  # Second hidden layer\n",
    "        self.relu2 = nn.ReLU()  # ReLU activation function\n",
    "        self.output_layer = nn.Linear(input_size //4 , 1)  # Output layer with linear activation\n",
    "        self.double()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNet(13)\n",
    "\n",
    "# 2) Create loss and optimizer\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 3) Training Loop\n",
    "#        Forward (compute predictions and loss)\n",
    "#        Backward (compute gradients)\n",
    "#        Update weights\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    # predict y (forward pass)\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_train, y_pred)\n",
    "\n",
    "    # Clear gradients for next loop\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate gradients (backward pass)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress every x number of epochs\n",
    "    if ((epoch+1) % 100 == 0):\n",
    "        print(f'Epoch: {epoch+1} loss: {loss.item()}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObgbZ308XwxAT/0fzUyBBf",
   "collapsed_sections": [],
   "name": "7 - Transformation PIpelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
