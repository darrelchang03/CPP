{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygODzEcxadQq"
      },
      "source": [
        "**Assignment 3 - Facial Recognition Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyL3TwmIbVWQ"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Gh0pQzPeXQgd",
        "outputId": "00315402-4a84-4e5f-9cb6-2c9688e0358f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train_target.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomRotation, RandomHorizontalFlip, GaussianBlur\n\u001b[0;32m     20\u001b[0m data_augmentation \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     21\u001b[0m     \u001b[39m# RandomRotation(degrees=15),\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     RandomHorizontalFlip(p\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m),\n\u001b[0;32m     23\u001b[0m     \u001b[39m# GaussianBlur(kernel_size=3)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m train_target \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m/content/train_target.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m     29\u001b[0m \u001b[39m# Creating own testing data to prevent having to submit to kaggle for testing error\u001b[39;00m\n\u001b[0;32m     30\u001b[0m train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/content/train_data.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mto_numpy()\n",
            "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_target.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datapoints\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision.transforms import RandomRotation, RandomHorizontalFlip, GaussianBlur\n",
        "\n",
        "# Data augmentation class\n",
        "data_augmentation = torch.nn.Sequential(\n",
        "    # RandomRotation(degrees=15),\n",
        "    RandomHorizontalFlip(p=0.3),\n",
        "    # GaussianBlur(kernel_size=3)\n",
        ")\n",
        "\n",
        "train_target = pd.read_csv(\"/content/train_target.csv\", header=None).to_numpy()\n",
        "\n",
        "\n",
        "# Creating own testing data to prevent having to submit to kaggle for testing error\n",
        "train_data = pd.read_csv(\"/content/train_data.csv\", header=None).to_numpy()\n",
        "train_data, test_dataA, train_target, test_targetA = train_test_split(\n",
        "    train_data, train_target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Converting training data/target to tensors and correct data-type\n",
        "train_target = torch.from_numpy(train_target).to('cuda')\n",
        "train_target = train_target.to(torch.int64)\n",
        "train_data = torch.from_numpy(train_data).to('cuda')\n",
        "train_data = train_data.to(torch.float32)\n",
        "\n",
        "\n",
        "# Data Augmentation Attempted not used in final\n",
        "\n",
        "# train_data = torch.nn.functional.normalize(train_data, dim=1)\n",
        "# num_samples = train_data.size(0)\n",
        "# train_data = train_data.view(num_samples, 1, 48, 48)\n",
        "# train_data = torch.stack([data_augmentation(img) for img in train_data])\n",
        "# train_data = train_data.view(num_samples, -1)\n",
        "\n",
        "\n",
        "# Kaggle testing data provided \n",
        "test = pd.read_csv(\"/content/test_data.csv\", header=None).to_numpy()\n",
        "test_data = torch.from_numpy(test).to('cuda')\n",
        "test_data = test_data.to(torch.float32)\n",
        "\n",
        "# Data loader for training\n",
        "dataloader = DataLoader(\n",
        "    dataset=torch.utils.data.TensorDataset(train_data, train_target),\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "# 1 x 48 x 48\n",
        "\n",
        "# Image Classifier Model Class\n",
        "class ImageClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        # convolutional layers\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        # fully connected layers\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(64 * 6 * 6, 128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.30),\n",
        "        nn.BatchNorm1d(128),\n",
        "\n",
        "        nn.Linear(128, 64),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.30),\n",
        "        nn.BatchNorm1d(64),\n",
        "\n",
        "        nn.Linear(64, 3),  # output layer with 3 neurons for 3 classes\n",
        "        nn.Softmax(dim=1)  # softmax to get the class probabilities\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "# Create instance, loss, and optimizer\n",
        "\n",
        "lr = .001\n",
        "epochs = 10\n",
        "\n",
        "clf = ImageClassifier().to('cuda')\n",
        "opt = Adam(clf.parameters(), lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for epoch in range(epochs):\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        batch_size = X_batch.size(0)\n",
        "        X_batch = X_batch.view(batch_size, 1, 48, 48)\n",
        "        y_pred = clf(X_batch)\n",
        "        y_batch = y_batch.squeeze()\n",
        "\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} loss is {loss.item()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "5VDfpaE8pxMK"
      },
      "outputs": [],
      "source": [
        "test_dataA = torch.from_numpy(test_dataA).to('cuda')\n",
        "test_dataA = test_dataA.to(torch.float32)\n",
        "batch_size = test_dataA.size(0)\n",
        "test_dataA = test_dataA.view(batch_size, 1, 48, 48)\n",
        "test_dataA = test_dataA.to(torch.float32)\n",
        "test_targetA = torch.from_numpy(test_targetA).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_qQBSa_iwXQ",
        "outputId": "eb8275e0-44d8-4232-883f-293de603ca68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.8083, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_predA = clf(test_dataA)\n",
        "test_targetA = test_targetA.squeeze()\n",
        "loss = loss_fn(test_predA, test_targetA)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZF-oYVdfGZi",
        "outputId": "6da18abb-8137-466b-95b4-92703f4c1d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Id  Category\n",
            "0        0         0\n",
            "1        1         0\n",
            "2        2         1\n",
            "3        3         1\n",
            "4        4         1\n",
            "...    ...       ...\n",
            "3960  3960         0\n",
            "3961  3961         1\n",
            "3962  3962         1\n",
            "3963  3963         2\n",
            "3964  3964         2\n",
            "\n",
            "[3965 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Processing Kaggle test data\n",
        "batch_size = test_data.size(0)\n",
        "test_data = test_data.view(batch_size, 1, 48, 48)\n",
        "test_data = test_data.to(torch.float32)\n",
        "\n",
        "# Formatting output to Kaggle format Id:Category\n",
        "test_pred = clf(test_data)\n",
        "predicted_classes = torch.argmax(test_pred, dim=1).cpu().numpy()\n",
        "df = pd.DataFrame({'Id': range(len(predicted_classes)), 'Category': predicted_classes})\n",
        "print(df)\n",
        "\n",
        "# Output save as file\n",
        "filepath = \"/content/predictions.csv\"\n",
        "df.to_csv(filepath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHJLfg7T3qHs",
        "outputId": "77e3bdde-58b6-4676-bec4-892c3f2bbb87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
